{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - lab exercise 1\n",
    "\n",
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string: str, tolower:bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename: str, limit:int=-1) -> list[list[str]]:\n",
    "    dataset =[]\n",
    "    with open(filename) as f:\n",
    "        line: str = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell load the first 5000 sentences in each review set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  10000  lines from  ../data/imdb/imdb.pos  /  1  lines discarded\n",
      "Load  10000  lines from  ../data/imdb/imdb.neg  /  3  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM = 10000\n",
    "txtfile = \"../data/imdb/imdb.pos\"  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"../data/imdb/imdb.neg\"  # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between train / dev / test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = postxt + negtxt # Combine all reviews in a list\n",
    "labels = [1] * len(postxt) + [0] * len(negtxt) # Create labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_train, txt_test_dev, label_train, label_test_dev = train_test_split(reviews, labels, # Splitting both reviews and labels\n",
    "                                                                        train_size=0.7, # Keeping 70 % of the dataset as training data\n",
    "                                                                        stratify=labels, # Using stratify to ensure both sets keep a 50/50 ratio betwwen positive and negative reviews \n",
    "                                                                        random_state=42) # Setting random_state for replicability\n",
    "\n",
    "\n",
    "txt_test, txt_dev, label_test, label_dev = train_test_split(txt_test_dev, label_test_dev, # Splitting test and dev sets\n",
    "                                                            train_size=0.5, # Keeping 15 % of the dataset as test/dev data\n",
    "                                                            stratify=label_test_dev, # Using stratify to ensure both sets keep a 50/50 ratio betwwen positive and negative reviews \n",
    "                                                            random_state=42) # Setting random_state for replicability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to Pytorch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences: list[list[str]]) -> dict[str, int]:\n",
    "    \"\"\"Generate the vocabulary used in a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[list[str]]): List of sentences. Sentences should already be split by words/tokens\n",
    "\n",
    "    Returns:\n",
    "        dict[str, int]: Vocabulary, in the form of a dictionnary where each word is assigned to an integer\n",
    "    \"\"\"\n",
    "    vocab = {'<UNK>' : 0, '<PAD>' : 1, '<BOS>' : 2, '<EOS>' : 3} # Initializing the vocab with an base tokens\n",
    "    index = 4 # Starting BOW from index 1 \n",
    "\n",
    "    for sentence in sentences: # Iterating through the sentences\n",
    "        for word in sentence: # Iterating throigh each word in the sentence\n",
    "            if word not in vocab: # If we didn't see the word already\n",
    "                vocab[word] = index # We had it to the vocabulary\n",
    "                index +=1 # We adjust the running index\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def sentences_to_tensor(sentences:list[list[str]], vocab:dict[str, int], padding_size:int) -> torch.Tensor:\n",
    "\n",
    "    tokenized_sentences = [\n",
    "        [vocab.get('<BOS>')] + [vocab.get(word, vocab.get('<UNK>')) for word in sentence] + [vocab.get('<EOS>')] # Create the list of tokenized word. We add <BOS> and <EOS> for delimiting the sentence. If the word is not in the vocab dictionnary, we had the encoding for the <UNK> token\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    tokenized_sentences = [\n",
    "        tokens[:padding_size] + [vocab.get('<PAD>')] * max(0, padding_size - len(tokens))\n",
    "        for tokens in tokenized_sentences\n",
    "    ]\n",
    "\n",
    "    tensor_data = torch.tensor(tokenized_sentences, dtype=torch.long)\n",
    "\n",
    "    return tensor_data\n",
    "\n",
    "vocab = build_vocab(txt_train)\n",
    "\n",
    "tensor_txt_train = sentences_to_tensor(txt_train, vocab, max([len(sentence) for sentence in reviews]))\n",
    "tensor_txt_dev = sentences_to_tensor(txt_dev, vocab, max([len(sentence) for sentence in reviews]))\n",
    "tensor_txt_test = sentences_to_tensor(txt_test, vocab, max([len(sentence) for sentence in reviews]))\n",
    "\n",
    "tensor_label_train = torch.tensor(label_train, dtype=torch.long)\n",
    "tensor_label_dev = torch.tensor(label_dev, dtype=torch.long)\n",
    "tensor_label_test = torch.tensor(label_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve classifier\n",
    "class Naive_classifier(nn.Module):\n",
    "    \"\"\"A simple naïve classification model. returns at random the class of a model.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward method for \"training\"\n",
    "\n",
    "        Args:\n",
    "            inputs (list[torch.Tensor]): batch input of the model\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: random predictions. -1 predicts a negative class, 1 a positive class\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        random.seed(42)\n",
    "        random_predictions = torch.tensor([random.randint(0, 1) for _ in range(batch_size)])\n",
    "        return random_predictions\n",
    "\n",
    "# BAG of word classifier\n",
    "class CBOW_classifier(nn.Module):\n",
    "    \"\"\"A simple Bag of words classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int) -> None:\n",
    "        \"\"\"Initialize the BagOfWords model\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int):Size of the vocabulary, to create the embedding space\n",
    "            embedding_dim (int): Dimension of the embedding space\n",
    "        \"\"\"\n",
    "        super(CBOW_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim //2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim //2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Batched inputs of the model\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: logits output of the model\n",
    "        \"\"\"\n",
    "        # Get embeddings and sum along the axis\n",
    "        embedded = self.embedding(inputs) # (batch_size, sentence_len, embedding_dim)\n",
    "        embedded = embedded.sum(dim=1) # (batch_size, embedding_dim)\n",
    "\n",
    "        # Pass through the MLP to get the logits outputs\n",
    "        logits = self.MLP(embedded) # (batch_size, 1)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class CNN_classifier(nn.Module):\n",
    "    \"\"\"Based on the paper \"Convolutional Neural Networks for Sentence Classification, Yoon Kim, 2014\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int, num_filters:int, filter_sizes:list[int], dropout:int=0.5) -> None:\n",
    "        \"\"\"Initialize the CNN model\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            embedsing_dim (int): Dimesnion of the embedding space\n",
    "            num_filters (int):Number of convolutional filters\n",
    "            filter_sizes (list[int]): List of sizes of the convolutional filters. Size 1 focus on unigrams, 2 on bigrams, 3 on trigrams ... \n",
    "            dropout (int, optional): _description_. Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        super(CNN_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim,\n",
    "                                             out_channels=num_filters,\n",
    "                                             kernel_size = fs)\n",
    "                                    for fs in filter_sizes])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(num_filters * len(filter_sizes), (num_filters * len(filter_sizes)) //2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear((num_filters * len(filter_sizes)) //2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs:torch.Tensor)-> torch.Tensor:\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Batch of input tensors\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: logits output of the model\n",
    "        \"\"\"\n",
    "\n",
    "        embedded = self.embedding(inputs) # (batch_size, sentence_len, embedding_dim) \n",
    "        embedded = embedded.permute(0,2,1) # (batch_size, embedding_dim, sentence_len)\n",
    "\n",
    "        out_conv = [F.relu(conv(embedded)).max(dim=2)[0]\n",
    "                    for conv in self.convs]\n",
    "        out_conv = torch.cat(out_conv, dim=1) # (batch_size, num-filters * len(filter_sizes))\n",
    "\n",
    "        out_conv = self.dropout(out_conv) # (batch_size, num-filters * len(filter_sizes))\n",
    "\n",
    "        logits = self.MLP(out_conv) # (batch_size, 1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model:nn.Module, train_loader:DataLoader, dev_loader:DataLoader, num_epochs:int, criterion:nn.Module, optimizer:optim.Optimizer, device:torch.device, patience:int=2, min_delta:float=0.001) -> None:\n",
    "    \"\"\"Training a model passed as input\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        train_loader (DataLoader): train set, as a DataLoader\n",
    "        dev_loader (DataLoader): dev set, as a DataLoader\n",
    "        num_epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss fuction\n",
    "        optimizer (optim.Optimizer): Optimizer for training\n",
    "        device (torch.device): Device to run the training on\n",
    "        patience (int): Number of epochs to wait before checking for early stopping. Defaults to 2.\n",
    "        min_delta (float): Minimum variation in dev loss to check for early stopping. Defaults to 0.001\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    best_dev_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for sentences, labels in train_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(sentences)\n",
    "            loss = criterion(logits.squeeze().float(), labels.float())\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Dev loop\n",
    "        model.eval()\n",
    "        dev_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for sentences, labels in dev_loader:\n",
    "                sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(sentences)\n",
    "                loss = criterion(logits.squeeze().float(), labels.float())\n",
    "                dev_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predictions = (torch.sigmoid(logits.squeeze()) > 0.5).long()\n",
    "                correct += (predictions == labels.long()).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        dev_loss /= len(dev_loader)\n",
    "\n",
    "        # Early stopping\n",
    "        if dev_loss < best_dev_loss - min_delta:\n",
    "            best_dev_loss = dev_loss\n",
    "            no_improvement_epochs = 0\n",
    "            print(\"Dev loss improved, saving current model.\")\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(\"Patience reache. Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.2f}, \"\n",
    "              f\"Dev Loss: {dev_loss:.2f}, \"\n",
    "              f\"Dev Accuraccy: {correct/total:.2f}\")\n",
    "        \n",
    "    print(\"Loading best model.\")\n",
    "    model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "    os.remove('best_model.pth')\n",
    "    print(\"Saved model deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss improved, saving current model.\n",
      "Epoch [1/10], Train Loss: 0.74, Dev Loss: 0.68, Dev Accuraccy: 0.59\n",
      "Epoch [2/10], Train Loss: 0.55, Dev Loss: 0.69, Dev Accuraccy: 0.63\n",
      "Dev loss improved, saving current model.\n",
      "Epoch [3/10], Train Loss: 0.45, Dev Loss: 0.67, Dev Accuraccy: 0.68\n",
      "Epoch [4/10], Train Loss: 0.38, Dev Loss: 0.70, Dev Accuraccy: 0.69\n",
      "Patience reache. Early stopping triggered.\n",
      "Loading best model.\n",
      "Saved model deleted.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dataset = TensorDataset(tensor_txt_train, tensor_label_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16)\n",
    "\n",
    "dev_dataset = TensorDataset(tensor_txt_dev, tensor_label_dev)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size = 16)\n",
    "\n",
    "\n",
    "BOW_model = CBOW_classifier(vocab_size=len(vocab), embedding_dim=128)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(BOW_model.parameters())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_model(BOW_model, train_loader, dev_loader, num_epochs, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss improved, saving current model.\n",
      "Epoch [1/10], Train Loss: 0.62, Dev Loss: 0.53, Dev Accuraccy: 0.74\n",
      "Dev loss improved, saving current model.\n",
      "Epoch [2/10], Train Loss: 0.49, Dev Loss: 0.46, Dev Accuraccy: 0.78\n",
      "Dev loss improved, saving current model.\n",
      "Epoch [3/10], Train Loss: 0.41, Dev Loss: 0.43, Dev Accuraccy: 0.79\n",
      "Dev loss improved, saving current model.\n",
      "Epoch [4/10], Train Loss: 0.35, Dev Loss: 0.42, Dev Accuraccy: 0.79\n",
      "Epoch [5/10], Train Loss: 0.30, Dev Loss: 0.43, Dev Accuraccy: 0.79\n",
      "Patience reache. Early stopping triggered.\n",
      "Loading best model.\n",
      "Saved model deleted.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dataset = TensorDataset(tensor_txt_train, tensor_label_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16)\n",
    "\n",
    "dev_dataset = TensorDataset(tensor_txt_dev, tensor_label_dev)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size = 16)\n",
    "\n",
    "\n",
    "CNN_model = CNN_classifier(vocab_size=len(vocab), embedding_dim = 128, num_filters=10, filter_sizes=[2, 3, 4])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_model(CNN_model, train_loader, dev_loader, num_epochs, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(model:nn.Module, test_loader:DataLoader) -> tuple[float, str | dict]:\n",
    "    \"\"\"Evaluation function\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to evaluate\n",
    "        test_data (DataLoader): Test set to evaluate the model on\n",
    "        test_labels (torch.Tensor): Test labels\n",
    "\n",
    "    Returns:\n",
    "        tuple[Float, str | dict]: accuarcy score and accuracy report from sklearn.metrics\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in test_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(sentences)\n",
    "            preds = (logits.sigmoid() > 0.5).long().tolist()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "\n",
    "    return accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Classifier (Random Predictions) Results\n",
      "Accuracy: 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.87      0.64      1500\n",
      "           1       0.50      0.12      0.20      1500\n",
      "\n",
      "    accuracy                           0.50      3000\n",
      "   macro avg       0.50      0.50      0.42      3000\n",
      "weighted avg       0.50      0.50      0.42      3000\n",
      "\n",
      "BOW Classifier Results\n",
      "Accuracy: 0.68\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.40      0.56      1500\n",
      "           1       0.62      0.96      0.75      1500\n",
      "\n",
      "    accuracy                           0.68      3000\n",
      "   macro avg       0.77      0.68      0.66      3000\n",
      "weighted avg       0.77      0.68      0.66      3000\n",
      "\n",
      "CNN Classifier Results\n",
      "Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      1500\n",
      "           1       0.82      0.75      0.78      1500\n",
      "\n",
      "    accuracy                           0.79      3000\n",
      "   macro avg       0.79      0.79      0.79      3000\n",
      "weighted avg       0.79      0.79      0.79      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(tensor_txt_test, tensor_label_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16)\n",
    "\n",
    "naive_model = Naive_classifier()\n",
    "naive_accuracy, naive_report = evaluate_model(naive_model, test_loader)\n",
    "print(\"Naive Classifier (Random Predictions) Results\")\n",
    "print(f\"Accuracy: {naive_accuracy:.2f}\")\n",
    "print(naive_report)\n",
    "\n",
    "BOW_model.eval()\n",
    "BOW_accuracy, BOW_report = evaluate_model(BOW_model, test_loader)\n",
    "print(\"BOW Classifier Results\")\n",
    "print(f\"Accuracy: {BOW_accuracy:.2f}\")\n",
    "print(BOW_report)\n",
    "\n",
    "CNN_model.eval()\n",
    "CNN_accuracy, CNN_report = evaluate_model(CNN_model, test_loader)\n",
    "print(\"CNN Classifier Results\")\n",
    "print(f\"Accuracy: {CNN_accuracy:.2f}\")\n",
    "print(CNN_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
