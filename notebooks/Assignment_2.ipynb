{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch._tensor import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Lab Assignment 2\n",
    "\n",
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/google\" # Folder path for the train/test/dev .tsv files\n",
    "\n",
    "train_data_raw = pd.read_csv(os.path.join(DATA_PATH, \"train.tsv\"), sep='\\t', header=None)[0].tolist()[0:5000]\n",
    "test_data_raw = pd.read_csv(os.path.join(DATA_PATH, \"test.tsv\"), sep='\\t', header=None)[0].tolist()[0:1000]\n",
    "dev_data_raw = pd.read_csv(os.path.join(DATA_PATH, \"dev.tsv\"), sep='\\t', header=None)[0].tolist()[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string: str, tolower:bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "train_data, test_data, dev_data = [], [], []\n",
    "\n",
    "for sentence in train_data_raw:\n",
    "    train_data.append(clean_str(sentence))\n",
    "for sentence in dev_data_raw:\n",
    "    dev_data.append(clean_str(sentence))\n",
    "for sentence in test_data_raw:\n",
    "    test_data.append(clean_str(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    \"\"\"Word dictionnary class.\n",
    "    \"\"\"\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words:set) -> None:\n",
    "        \"\"\"Initialize a word dictionnary\n",
    "\n",
    "        Args:\n",
    "            words (set): set of all words in a dataset\n",
    "        \"\"\"\n",
    "        assert type(words) == set\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "\n",
    "    def word_to_id(self, word:str) -> int:\n",
    "        \"\"\"Return the integer associated with a word.\n",
    "\n",
    "        Args:\n",
    "            word (str): word\n",
    "\n",
    "        Returns:\n",
    "            int: index of the word in the vocabulary\n",
    "        \"\"\"\n",
    "        return self.word_to_idx[word]\n",
    "    \n",
    "    def id_to_word(self, idx:int) -> str:\n",
    "        \"\"\"Return the word associated with an integer.\n",
    "\n",
    "        Args:\n",
    "            idx (int): integer\n",
    "\n",
    "        Returns:\n",
    "            str: word at that index in the word dictionnary\n",
    "        \"\"\"\n",
    "        return self.idx_to_word[idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compute length of the dictionnary\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dictionnary of words\n",
    "        \"\"\"\n",
    "        return len(self.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 8159\n",
      "[(0, 'delight'), (1, 'rosa'), (2, 'adhd'), (3, 'correlated'), (4, 'harvard')]\n"
     ]
    }
   ],
   "source": [
    "train_words = set()\n",
    "\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence.split(\" \"))\n",
    "\n",
    "train_words.update([\"<bos>\", \"<eos>\", \"<unk>\", \"<pad>\"])\n",
    "\n",
    "word_dict = WordDict(train_words)\n",
    "\n",
    "print(\"Number of words :\", len(word_dict))\n",
    "print(list(word_dict.idx_to_word.items())[0:5]) # Excerpt of the idx_to_word dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural N-Gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):\n",
    "    \"\"\"Dataset for training the N-gram model, based on the Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences:list[str], vocab:WordDict, context_size:int) -> None:\n",
    "        \"\"\"N-gram Dataset to use with the DataLoader feature of torch during training\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): List of sentences\n",
    "            vocab (WordDict): _description_\n",
    "            context_size (int): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = [\"<bos>\"] * context_size + sentence.split(\" \") + [\"<eos>\"] # Pad the sentence and extract words. We pad the sentence with context_size <bos> tokens for generating the first word.\n",
    "            indices = [vocab.word_to_id(word) if word in vocab.word_to_idx.keys() else vocab.word_to_id('<unk>') for word in tokens] # Tokenize the sentence using the WordDict\n",
    "            for i in range(context_size, len(indices)):\n",
    "                self.data.append((torch.tensor(indices[i-context_size:i]),\n",
    "                                              torch.tensor(indices[i]))) # Append the training data with a tuple ([word_1, word_2 ,...], word_n)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compute len of dataset (necessary)\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Return a tuple of tensor, containing encodings of the n-grams and next word to predict\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the item in the dataset\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Tuple with n-gram data and next word in the sentence\n",
    "        \"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dataset length : 74083\n",
      "Training item example : (tensor([ 463, 3226, 2409,  398, 2303]), tensor(5439))\n",
      "Training sentence example : youtube and outrage drama is ; Target word example : super\n"
     ]
    }
   ],
   "source": [
    "# Define the context size (e.g., 2 for bigrams)\n",
    "CONTEXT_SIZE = 5  # For n-grams, context_size = n - 1\n",
    "\n",
    "# Dataset\n",
    "ngram_train_dataset = NGramDataset(train_data, word_dict, CONTEXT_SIZE)\n",
    "ngram_dev_dataset = NGramDataset(dev_data, word_dict, CONTEXT_SIZE)\n",
    "ngram_test_dataset = NGramDataset(test_data, word_dict, CONTEXT_SIZE)\n",
    "\n",
    "print(\"Train_dataset length :\", len(ngram_train_dataset))\n",
    "print(\"Training item example :\", ngram_train_dataset[123]) # Example of item in the training data\n",
    "print(\"Training sentence example :\", \" \".join([word_dict.id_to_word(id) for id in ngram_train_dataset[123][0].tolist()]), \"; Target word example :\", word_dict.id_to_word(ngram_train_dataset[123][1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNGramModel(nn.Module):\n",
    "    \"\"\"Class for the Neural N-gram module, based on the Pytorch based class.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embed_size:int, context_size:int, hidden_size:int) -> None:\n",
    "        \"\"\"Initialize the Neural N-gram model\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of words in the dictionnary\n",
    "            embed_size (int): Embedding size\n",
    "            context_size (int): Number \n",
    "            hidden_size (int): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input of the model (batched), size (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: output of the model, in the form of a (vocab_size, x.shape(1)) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        embeds = self.embeddings(x) # (batch_size, context_size, embed_size)\n",
    "        embeds = embeds.sum(dim=1) # (batch_size, embed_size)\n",
    "\n",
    "        hidden = self.dropout(F.relu(self.fc1(embeds))) # (batch_size, hidden_size)\n",
    "        output = self.fc2(hidden) # (batch_size, vocab_size)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 6.72, Dev Loss: 6.58\n",
      "Epoch [2/5], Train Loss: 6.27, Dev Loss: 6.68\n",
      "Epoch [3/5], Train Loss: 6.04, Dev Loss: 6.85\n",
      "Epoch [4/5], Train Loss: 5.83, Dev Loss: 7.10\n",
      "Epoch [5/5], Train Loss: 5.64, Dev Loss: 7.44\n"
     ]
    }
   ],
   "source": [
    "# Model and training parameters\n",
    "NGRAM_EMBED_SIZE = 64\n",
    "NGRAM_HIDDEN_SIZE = 128\n",
    "NGRAM_BATCH_SIZE = 32\n",
    "NGRAM_EPOCHS = 5\n",
    "\n",
    "# DataLoader\n",
    "ngram_train_loader = DataLoader(ngram_train_dataset, batch_size=NGRAM_BATCH_SIZE, shuffle=True)\n",
    "ngram_dev_loader = DataLoader(ngram_dev_dataset, batch_size=NGRAM_BATCH_SIZE)\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "vocab_size = len(word_dict)\n",
    "ngram_model = NeuralNGramModel(vocab_size, NGRAM_EMBED_SIZE, CONTEXT_SIZE, NGRAM_HIDDEN_SIZE)\n",
    "ngram_criterion = nn.CrossEntropyLoss(ignore_index=word_dict.word_to_id('<pad>')) # We can see the problem as a multi-label classification problem, classyfying among all possibles words in the vocab\n",
    "ngram_optimizer = optim.Adam(ngram_model.parameters())\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(NGRAM_EPOCHS):\n",
    "    ngram_model.train()\n",
    "    ngram_train_loss = 0\n",
    "\n",
    "    # Train loop\n",
    "    for context, target in ngram_train_loader:\n",
    "\n",
    "        # Forward pass\n",
    "        output = ngram_model(context)\n",
    "        loss = ngram_criterion(output, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        ngram_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ngram_optimizer.step()\n",
    "\n",
    "        ngram_train_loss += loss.item()\n",
    "    \n",
    "    # Dev loop\n",
    "    ngram_model.eval()\n",
    "    ngram_dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in ngram_dev_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            output = ngram_model(context)\n",
    "            loss = ngram_criterion(output, target)\n",
    "            ngram_dev_loss += loss.item()\n",
    "\n",
    "    ngram_train_loss /= len(ngram_train_loader)\n",
    "    ngram_dev_loss /= len(ngram_dev_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NGRAM_EPOCHS}], \"\n",
    "            f\"Train Loss: {ngram_train_loss:.2f}, \"\n",
    "            f\"Dev Loss: {ngram_dev_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic generated sentence : i love\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "ngram_model.eval()\n",
    "\n",
    "context = [\"<bos>\"] * CONTEXT_SIZE\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "generated_sequence = context[:]\n",
    "while generated_sequence[-1] != \"<eos>\":\n",
    "    context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence[-(CONTEXT_SIZE-1):]], dtype = torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = ngram_model(context_indices)\n",
    "        probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "    predicted_index = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_word = word_dict.id_to_word(predicted_index)\n",
    "    generated_sequence.append(predicted_word)\n",
    "\n",
    "print(\"Deterministic generated sentence :\", \" \".join(generated_sequence[CONTEXT_SIZE:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random generated sequence : how served it something feels it have him 2\n",
      "Random generated sequence : like mrs option for if to shoe the fun was well trip high\n",
      "Random generated sequence : used gonna\n",
      "Random generated sequence : should recoiled ! do have so s can back\n",
      "Random generated sequence : phrased times tsa door said ? i\n",
      "Random generated sequence : eat emotional matter my i ) t but appreciate for y started would that even this have ever one i think hes do to did n't the ruined have me so mess are place before what for people as explain find 've lucky\n",
      "Random generated sequence : hello friend then go how same him it i here like it but on his best calling\n",
      "Random generated sequence : where needs you this from , please is actually possible going become were mother woman but word xd like so , there is this partner section great the ?\n",
      "Random generated sequence : , congrats sounds i need to time never attractive pay in if\n",
      "Random generated sequence : reddit once man people welcome was the ? that you lucky getting smile for posted after ? going 's the simply perspective so dude yet guess at back\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "ngram_model.eval()\n",
    "\n",
    "context = [\"<bos>\"] * CONTEXT_SIZE\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "for _ in range(10):\n",
    "    generated_sequence = context[:]\n",
    "    while generated_sequence[-1] != \"<eos>\":\n",
    "        context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence[-(CONTEXT_SIZE-1):]], dtype = torch.long).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = ngram_model(context_indices)\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "        predicted_index = torch.multinomial(probabilities.squeeze(), num_samples=1).item()\n",
    "        predicted_word = word_dict.id_to_word(predicted_index)\n",
    "        generated_sequence.append(predicted_word)\n",
    "\n",
    "    print(\"Random generated sequence :\", \" \".join(generated_sequence[CONTEXT_SIZE:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-based Autoregressive Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    \"\"\"Dataset for training the LSTM model, based on the Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences:list[str], vocab:WordDict) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): List od sentences.\n",
    "            vocab (WordDict): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = [\"<bos>\"] + sentence.split(\" \") + [\"<eos>\"]\n",
    "            indices = [vocab.word_to_id(word) if word in vocab.word_to_idx.keys() else vocab.word_to_id('<unk>') for word in tokens] # Tokenize the sentence using the WordDict\n",
    "            \n",
    "            self.data.append((torch.tensor(indices),\n",
    "                              torch.tensor(indices[1:]+[self.vocab.word_to_id('<pad>')])))\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compute len of dataset (necessary)\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Return a tuple of tensor, containing encodings of the n-grams and next word to predict\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the item in the dataset\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Tuple with sentence and shifted sentence for generation\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch:list[tuple]) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Custom collate function for dynamically padding sentences\n",
    "\n",
    "    Args:\n",
    "        batch (list[tuple]): batch of a sentence Dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: Dynamicallt padded sentences\n",
    "    \"\"\"\n",
    "    sentences, shifted_sentences = zip(*batch)\n",
    "\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=word_dict.word_to_id('<pad>'))\n",
    "    padded_shifted_sentences = pad_sequence(shifted_sentences, batch_first=True, padding_value=word_dict.word_to_id('<pad>'))\n",
    "\n",
    "    return padded_sentences, padded_shifted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dataset length : 5000\n",
      "Training item example : (tensor([7151, 7733, 6294, 6591,  152, 5815,  474, 3748, 3621, 2914]), tensor([7733, 6294, 6591,  152, 5815,  474, 3748, 3621, 2914, 1685]))\n",
      "Training sentence example : <bos> three words , no subtlety dude stop seriously <eos>  ; Target sentence example : three words , no subtlety dude stop seriously <eos> <pad>\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "lstm_train_dataset = LSTMDataset(train_data, word_dict)\n",
    "lstm_dev_dataset = LSTMDataset(dev_data, word_dict)\n",
    "\n",
    "print(\"Train_dataset length :\", len(lstm_train_dataset))\n",
    "print(\"Training item example :\", lstm_train_dataset[123]) # Example of item in the training data\n",
    "print(\"Training sentence example :\", \" \".join([word_dict.id_to_word(id) for id in lstm_train_dataset[123][0].tolist()]), \" ; Target sentence example :\", \" \".join([word_dict.id_to_word(id) for id in lstm_train_dataset[123][1].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Class for the LSTM module, based on the Pytorch based class.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embed_size:int, hidden_size:int, dropout_prob:float=0.3) -> None:\n",
    "        \"\"\"Initialize the LSTM\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of words in the dictionnary\n",
    "            embed_size (int): Embedding size\n",
    "            hidden_size (int): Size of the hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def variational_dropout(self, input:Tensor) -> Tensor:\n",
    "        if self.training: # Apply variational dropout only during training\n",
    "            mask = (torch.rand_like(input) > self.dropout_prob).float().to(input.device)\n",
    "            mask = mask.div_(1.0 - self.dropout_prob)\n",
    "            return input * mask\n",
    "        return input # If not in training, return input\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input of the model (batched), size (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: output of the model, in the form of a (vocab_size, x.shape(1)) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        embeds = self.embeddings(x) # (batch_size, seq_len, embed_size)\n",
    "        embeds = self.variational_dropout(embeds) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        output, _ = self.lstm(embeds) # (batch_size, seq_len, hidden_size)\n",
    "        output = self.variational_dropout(output) # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        output = self.fc(output) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 6.98, Dev Loss: 0.00\n",
      "Epoch [2/10], Train Loss: 6.31, Dev Loss: 0.00\n",
      "Epoch [3/10], Train Loss: 6.17, Dev Loss: 0.00\n",
      "Epoch [4/10], Train Loss: 6.03, Dev Loss: 0.00\n",
      "Epoch [5/10], Train Loss: 5.91, Dev Loss: 0.00\n",
      "Epoch [6/10], Train Loss: 5.81, Dev Loss: 0.00\n",
      "Epoch [7/10], Train Loss: 5.72, Dev Loss: 0.00\n",
      "Epoch [8/10], Train Loss: 5.63, Dev Loss: 0.00\n",
      "Epoch [9/10], Train Loss: 5.56, Dev Loss: 0.00\n",
      "Epoch [10/10], Train Loss: 5.49, Dev Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Model and training parameters\n",
    "LSTM_EMBED_SIZE = 64\n",
    "LSTM_HIDDEN_SIZE = 128\n",
    "LSTM_BATCH_SIZE = 32\n",
    "LSTM_EPOCHS = 10\n",
    "\n",
    "# DataLoader\n",
    "lstm_train_loader = DataLoader(lstm_train_dataset, batch_size=LSTM_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "lstm_dev_loader = DataLoader(lstm_dev_dataset, batch_size=LSTM_BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "vocab_size = len(word_dict)\n",
    "lstm_model = LSTMModel(vocab_size, LSTM_EMBED_SIZE, LSTM_HIDDEN_SIZE)\n",
    "lstm_criterion = nn.CrossEntropyLoss(ignore_index=word_dict.word_to_id('<pad>')) # We can see the problem as a multi-label classification problem, classyfying among all possibles words in the vocab\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(LSTM_EPOCHS):\n",
    "    lstm_model.train()\n",
    "    lstm_train_loss = 0\n",
    "\n",
    "    # Train loop\n",
    "    for context, target in lstm_train_loader:\n",
    "\n",
    "        # Forward pass\n",
    "        output = lstm_model(context)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output = output.view(-1, vocab_size) # (batch_size *  seq_len, vocab_size)\n",
    "        target = target.view(-1) # (batch_size * seq_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = lstm_criterion(output, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        lstm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "        lstm_train_loss += loss.item()\n",
    "    \n",
    "    # Dev loop\n",
    "    lstm_model.eval()\n",
    "    lstm_dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in lstm_dev_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            output = lstm_model(context)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, vocab_size) # (batch_size *  seq_len, vocab_size)\n",
    "            target = target.view(-1) # (batch_size * seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = lstm_criterion(output, target)\n",
    "\n",
    "    lstm_train_loss /= len(lstm_train_loader)\n",
    "    lstm_dev_loss /= len(lstm_dev_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{LSTM_EPOCHS}], \"\n",
    "            f\"Train Loss: {lstm_train_loss:.2f}, \"\n",
    "            f\"Dev Loss: {lstm_dev_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic generated sentence : i m not a lot of the same of the same\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "lstm_model.eval()\n",
    "\n",
    "context = [\"<bos>\"]\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "generated_sequence = context[:]\n",
    "while generated_sequence[-1] != \"<eos>\" and len(generated_sequence) < MAX_LEN:\n",
    "    context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence], dtype = torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = lstm_model(context_indices)[:, -1]\n",
    "        probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "    predicted_index = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_word = word_dict.id_to_word(predicted_index)\n",
    "    generated_sequence.append(predicted_word)\n",
    "\n",
    "print(\"Deterministic generated sentence :\", \" \".join(generated_sequence[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random generated sentence : name cheese has to have 2 team spun thing or people\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random generated sentence : this for the name , are name\n",
      "Random generated sentence : thanks i ve n't knowing\n",
      "Random generated sentence : they can the wholesome for ? it is a lot , oh i would mean good personalities happen\n",
      "Random generated sentence : ditto ( for r handle in off you does\n",
      "Random generated sentence : they believe play real btw to a over of half deja stealing\n",
      "Random generated sentence : that happened solar aside\n",
      "Random generated sentence : no own name is a place he tiring if they 're a page\n",
      "Random generated sentence : i am hi ( what means i visibly shit believe from our position never expected that and odd for him\n",
      "Random generated sentence : why starter laugh someone brings nothing say my 's people\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "ngram_model.eval()\n",
    "\n",
    "context = [\"<bos>\"]\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "for _ in range(10):\n",
    "    generated_sequence = context[:]\n",
    "    while generated_sequence[-1] != \"<eos>\" and len(generated_sequence) < MAX_LEN:\n",
    "        context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence], dtype = torch.long).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = lstm_model(context_indices)[:, -1]\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "        predicted_index = torch.multinomial(probabilities.squeeze(), num_samples=1).item()\n",
    "        predicted_word = word_dict.id_to_word(predicted_index)\n",
    "        generated_sequence.append(predicted_word)\n",
    "\n",
    "    print(\"Random generated sentence :\", \" \".join(generated_sequence[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "    \"\"\"Perplexity computation\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Init method.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Rset method.\n",
    "        \"\"\"\n",
    "        self.log_sum = 0\n",
    "        self.total_words = 0\n",
    "\n",
    "    def add_sentence(self, log_probs) -> None:\n",
    "        \"\"\"Compute values for one sentence and store it in the class.\n",
    "        \"\"\"\n",
    "        self.log_sum += log_probs.sum().item()\n",
    "        self.total_words += log_probs.size(0)\n",
    "\n",
    "    def compute_perplexity(self) -> float:\n",
    "        \"\"\"Compute full Perplexity\n",
    "\n",
    "        Returns:\n",
    "            float: Final perplexity\n",
    "        \"\"\"\n",
    "        return math.exp(-self.log_sum / self.total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Model Perplexity : 1890.58\n"
     ]
    }
   ],
   "source": [
    "ngram_model.eval()\n",
    "perplexity_object = Perplexity()\n",
    "\n",
    "# Dataset & DataLoader\n",
    "ngram_test_dataset = NGramDataset(test_data, word_dict, CONTEXT_SIZE)\n",
    "ngram_test_loader = DataLoader(ngram_test_dataset, batch_size=NGRAM_BATCH_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context, target in ngram_test_loader:\n",
    "        output = ngram_model(context)\n",
    "        log_probs = torch.log_softmax(output, dim=1)\n",
    "        for i in range(len(target)):\n",
    "            perplexity_object.add_sentence(log_probs[i][target[i]].unsqueeze(0))\n",
    "\n",
    "perplexity = perplexity_object.compute_perplexity()\n",
    "\n",
    "print(f\"N-Gram Model Perplexity : {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Perplexity : 435.29\n"
     ]
    }
   ],
   "source": [
    "lstm_model.eval()\n",
    "perplexity_object.reset()\n",
    "\n",
    "# Dataset & DataLoader\n",
    "lstm_test_dataset = LSTMDataset(test_data, word_dict)\n",
    "lstm_test_loader = DataLoader(lstm_test_dataset, batch_size=LSTM_BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context, target in lstm_test_loader:\n",
    "        output = lstm_model(context)\n",
    "        log_probs = torch.log_softmax(output, dim=2)\n",
    "\n",
    "        log_probs = log_probs.view(-1, vocab_size)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        mask = (target != word_dict.word_to_id('<pad>'))\n",
    "        log_probs = log_probs[torch.arange(len(target))[mask], target[mask]]\n",
    "\n",
    "        perplexity_object.add_sentence(log_probs)\n",
    "\n",
    "perplexity = perplexity_object.compute_perplexity()\n",
    "\n",
    "print(f\"LSTM Model Perplexity : {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
