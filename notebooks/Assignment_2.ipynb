{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch._tensor import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Lab Assignment 2\n",
    "\n",
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/google\" # Folder path for the train/test/dev .tsv files\n",
    "\n",
    "train_data_raw = pd.read_csv(os.path.join(DATA_PATH, \"train.tsv\"), sep='\\t', header=None)[0].tolist()[:5000] # Necessary to take a subset of the dataset due to computation constraints\n",
    "test_data_raw = pd.read_csv(os.path.join(DATA_PATH, \"test.tsv\"), sep='\\t', header=None)[0].tolist()[:500]\n",
    "dev_data_raw = pd.read_csv(os.path.join(DATA_PATH, \"dev.tsv\"), sep='\\t', header=None)[0].tolist()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string: str, tolower:bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "train_data, test_data, dev_data = [], [], []\n",
    "\n",
    "for sentence in train_data_raw:\n",
    "    train_data.append(clean_str(sentence))\n",
    "for sentence in dev_data_raw:\n",
    "    dev_data.append(clean_str(sentence))\n",
    "for sentence in test_data_raw:\n",
    "    test_data.append(clean_str(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    \"\"\"Word dictionnary class.\n",
    "    \"\"\"\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words:set) -> None:\n",
    "        \"\"\"Initialize a word dictionnary\n",
    "\n",
    "        Args:\n",
    "            words (set): set of all words in a dataset\n",
    "        \"\"\"\n",
    "        assert type(words) == set\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "\n",
    "    def word_to_id(self, word:str) -> int:\n",
    "        \"\"\"Return the integer associated with a word.\n",
    "\n",
    "        Args:\n",
    "            word (str): word\n",
    "\n",
    "        Returns:\n",
    "            int: index of the word in the vocabulary\n",
    "        \"\"\"\n",
    "        return self.word_to_idx[word]\n",
    "    \n",
    "    def id_to_word(self, idx:int) -> str:\n",
    "        \"\"\"Return the word associated with an integer.\n",
    "\n",
    "        Args:\n",
    "            idx (int): integer\n",
    "\n",
    "        Returns:\n",
    "            str: word at that index in the word dictionnary\n",
    "        \"\"\"\n",
    "        return self.idx_to_word[idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compute length of the dictionnary\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dictionnary of words\n",
    "        \"\"\"\n",
    "        return len(self.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 8159\n",
      "[(0, 'caught'), (1, 'corpus'), (2, 'dragged'), (3, 'freaking'), (4, 'racism')]\n"
     ]
    }
   ],
   "source": [
    "train_words = set()\n",
    "\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence.split(\" \"))\n",
    "\n",
    "train_words.update([\"<bos>\", \"<eos>\", \"<unk>\", \"<pad>\"])\n",
    "\n",
    "word_dict = WordDict(train_words)\n",
    "\n",
    "print(\"Number of words :\", len(word_dict))\n",
    "print(list(word_dict.idx_to_word.items())[0:5]) # Excerpt of the idx_to_word dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural N-Gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):\n",
    "    \"\"\"Dataset for training the N-gram model, based on the Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences:list[str], vocab:WordDict, context_size:int) -> None:\n",
    "        \"\"\"N-gram Dataset to use with the DataLoader feature of torch during training\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): List of sentences\n",
    "            vocab (WordDict): _description_\n",
    "            context_size (int): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = [\"<bos>\"] * context_size + sentence.split(\" \") + [\"<eos>\"] # Pad the sentence and extract words. We pad the sentence with context_size <bos> tokens for generating the first word.\n",
    "            indices = [vocab.word_to_id(word) if word in vocab.word_to_idx.keys() else vocab.word_to_id('<unk>') for word in tokens] # Tokenize the sentence using the WordDict\n",
    "            for i in range(context_size, len(indices)):\n",
    "                self.data.append((torch.tensor(indices[i-context_size:i]),\n",
    "                                              torch.tensor(indices[i]))) # Append the training data with a tuple ([word_1, word_2 ,...], word_n)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compute len of dataset (necessary)\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Return a tuple of tensor, containing encodings of the n-grams and next word to predict\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the item in the dataset\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Tuple with n-gram data and next word in the sentence\n",
    "        \"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dataset length : 74083\n",
      "Training item example : (tensor([4547, 1234,  943, 2189, 2597]), tensor(2550))\n",
      "Training sentence example : youtube and outrage drama is ; Target word example : super\n"
     ]
    }
   ],
   "source": [
    "# Define the context size (e.g., 2 for bigrams)\n",
    "CONTEXT_SIZE = 5  # For n-grams, context_size = n - 1\n",
    "\n",
    "# Dataset\n",
    "ngram_train_dataset = NGramDataset(train_data, word_dict, CONTEXT_SIZE)\n",
    "ngram_dev_dataset = NGramDataset(dev_data, word_dict, CONTEXT_SIZE)\n",
    "ngram_test_dataset = NGramDataset(test_data, word_dict, CONTEXT_SIZE)\n",
    "\n",
    "print(\"Train_dataset length :\", len(ngram_train_dataset))\n",
    "print(\"Training item example :\", ngram_train_dataset[123]) # Example of item in the training data\n",
    "print(\"Training sentence example :\", \" \".join([word_dict.id_to_word(id) for id in ngram_train_dataset[123][0].tolist()]), \"; Target word example :\", word_dict.id_to_word(ngram_train_dataset[123][1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNGramModel(nn.Module):\n",
    "    \"\"\"Class for the Neural N-gram module, based on the Pytorch based class.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embed_size:int, context_size:int, hidden_size:int) -> None:\n",
    "        \"\"\"Initialize the Neural N-gram model\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of words in the dictionnary\n",
    "            embed_size (int): Embedding size\n",
    "            context_size (int): Number \n",
    "            hidden_size (int): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size * context_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input of the model (batched), size (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: output of the model, in the form of a (vocab_size, x.shape(1)) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        embeds = self.embeddings(x) # (batch_size, context_size, embed_size)\n",
    "        embeds = embeds.view(embeds.size(0), -1) # (batch_size, embed_size * context_size)\n",
    "\n",
    "        hidden = self.dropout(F.relu(self.fc1(embeds))) # (batch_size, hidden_size)\n",
    "        output = self.fc2(hidden) # (batch_size, vocab_size)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 6.68, Dev Loss: 6.50\n",
      "Epoch [2/10], Train Loss: 6.23, Dev Loss: 6.54\n",
      "Epoch [3/10], Train Loss: 5.99, Dev Loss: 6.70\n",
      "Epoch [4/10], Train Loss: 5.79, Dev Loss: 6.99\n",
      "Epoch [5/10], Train Loss: 5.62, Dev Loss: 7.25\n",
      "Epoch [6/10], Train Loss: 5.47, Dev Loss: 7.70\n",
      "Epoch [7/10], Train Loss: 5.34, Dev Loss: 8.09\n",
      "Epoch [8/10], Train Loss: 5.24, Dev Loss: 8.46\n",
      "Epoch [9/10], Train Loss: 5.15, Dev Loss: 8.93\n",
      "Epoch [10/10], Train Loss: 5.07, Dev Loss: 9.40\n"
     ]
    }
   ],
   "source": [
    "# Model and training parameters\n",
    "NGRAM_EMBED_SIZE = 128\n",
    "NGRAM_HIDDEN_SIZE = 64\n",
    "NGRAM_BATCH_SIZE = 32\n",
    "NGRAM_EPOCHS = 10\n",
    "\n",
    "# DataLoader\n",
    "ngram_train_loader = DataLoader(ngram_train_dataset, batch_size=NGRAM_BATCH_SIZE, shuffle=True)\n",
    "ngram_dev_loader = DataLoader(ngram_dev_dataset, batch_size=NGRAM_BATCH_SIZE)\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "vocab_size = len(word_dict)\n",
    "ngram_model = NeuralNGramModel(vocab_size, NGRAM_EMBED_SIZE, CONTEXT_SIZE, NGRAM_HIDDEN_SIZE)\n",
    "ngram_criterion = nn.CrossEntropyLoss(ignore_index=word_dict.word_to_id('<pad>')) # We can see the problem as a multi-label classification problem, classyfying among all possibles words in the vocab\n",
    "ngram_optimizer = optim.Adam(ngram_model.parameters())\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(NGRAM_EPOCHS):\n",
    "    ngram_model.train()\n",
    "    ngram_train_loss = 0\n",
    "\n",
    "    # Train loop\n",
    "    for context, target in ngram_train_loader:\n",
    "\n",
    "        # Forward pass\n",
    "        output = ngram_model(context)\n",
    "        loss = ngram_criterion(output, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        ngram_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ngram_optimizer.step()\n",
    "\n",
    "        ngram_train_loss += loss.item()\n",
    "    \n",
    "    # Dev loop\n",
    "    ngram_model.eval()\n",
    "    ngram_dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in ngram_dev_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            output = ngram_model(context)\n",
    "            loss = ngram_criterion(output, target)\n",
    "            ngram_dev_loss += loss.item()\n",
    "\n",
    "    ngram_train_loss /= len(ngram_train_loader)\n",
    "    ngram_dev_loss /= len(ngram_dev_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NGRAM_EPOCHS}], \"\n",
    "            f\"Train Loss: {ngram_train_loss:.2f}, \"\n",
    "            f\"Dev Loss: {ngram_dev_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic generated sentence : i have the\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "MAX_LEN = 25\n",
    "\n",
    "ngram_model.eval()\n",
    "\n",
    "context = [\"<bos>\"] * CONTEXT_SIZE\n",
    "\n",
    "\n",
    "generated_sequence = context[:]\n",
    "while generated_sequence[-1] != \"<eos>\" and len(generated_sequence) <= MAX_LEN:\n",
    "    context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence[-CONTEXT_SIZE:]], dtype = torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = ngram_model(context_indices)\n",
    "        probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "    predicted_index = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_word = word_dict.id_to_word(predicted_index)\n",
    "    generated_sequence.append(predicted_word)\n",
    "\n",
    "print(\"Deterministic generated sentence :\", \" \".join(generated_sequence[CONTEXT_SIZE:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random generated sequence : sorry for seeing one and no brushing from players people do you got guidance\n",
      "Random generated sequence : no , but the women makeup , , before d do to the talent\n",
      "Random generated sequence : that wouldn t lose the vehicles , so i dont see an pills ! an ago !\n",
      "Random generated sequence : won that water started lists\n",
      "Random generated sequence : it does n't dumb the considering is delusional about lot\n",
      "Random generated sequence : name ! , i'm seen !\n",
      "Random generated sequence : so so game that in back ever sometimes language\n",
      "Random generated sequence : that won i used some have men is make going with stuff\n",
      "Random generated sequence : very lol awesome , that , when all control\n",
      "Random generated sequence : you ?\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "ngram_model.eval()\n",
    "\n",
    "context = [\"<bos>\"] * CONTEXT_SIZE\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "for _ in range(10):\n",
    "    generated_sequence = context[:]\n",
    "    while generated_sequence[-1] != \"<eos>\" and len(generated_sequence) <= MAX_LEN:\n",
    "        context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence[-CONTEXT_SIZE:]], dtype = torch.long).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = ngram_model(context_indices)\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "        predicted_index = torch.multinomial(probabilities.squeeze(), num_samples=1).item()\n",
    "        predicted_word = word_dict.id_to_word(predicted_index)\n",
    "        generated_sequence.append(predicted_word)\n",
    "\n",
    "    print(\"Random generated sequence :\", \" \".join(generated_sequence[CONTEXT_SIZE:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-based Autoregressive Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    \"\"\"Dataset for training the LSTM model, based on the Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences:list[str], vocab:WordDict) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): List od sentences.\n",
    "            vocab (WordDict): _description_\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = [\"<bos>\"] + sentence.split(\" \") + [\"<eos>\"]\n",
    "            indices = [vocab.word_to_id(word) if word in vocab.word_to_idx.keys() else vocab.word_to_id('<unk>') for word in tokens] # Tokenize the sentence using the WordDict\n",
    "            \n",
    "            self.data.append((torch.tensor(indices),\n",
    "                              torch.tensor(indices[1:]+[self.vocab.word_to_id('<pad>')])))\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Compute len of dataset (necessary)\n",
    "\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Return a tuple of tensor, containing encodings of the n-grams and next word to predict\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the item in the dataset\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Tuple with sentence and shifted sentence for generation\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch:list[tuple]) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Custom collate function for dynamically padding sentences\n",
    "\n",
    "    Args:\n",
    "        batch (list[tuple]): batch of a sentence Dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: Dynamicallt padded sentences\n",
    "    \"\"\"\n",
    "    sentences, shifted_sentences = zip(*batch)\n",
    "\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=word_dict.word_to_id('<pad>'))\n",
    "    padded_shifted_sentences = pad_sequence(shifted_sentences, batch_first=True, padding_value=word_dict.word_to_id('<pad>'))\n",
    "\n",
    "    return padded_sentences, padded_shifted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dataset length : 5000\n",
      "Training item example : (tensor([7046, 3383, 6047, 1204, 4540, 7763, 3047, 1079, 7365, 6040]), tensor([3383, 6047, 1204, 4540, 7763, 3047, 1079, 7365, 6040, 5820]))\n",
      "Training sentence example : <bos> three words , no subtlety dude stop seriously <eos>  ; Target sentence example : three words , no subtlety dude stop seriously <eos> <pad>\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "lstm_train_dataset = LSTMDataset(train_data, word_dict)\n",
    "lstm_dev_dataset = LSTMDataset(dev_data, word_dict)\n",
    "\n",
    "print(\"Train_dataset length :\", len(lstm_train_dataset))\n",
    "print(\"Training item example :\", lstm_train_dataset[123]) # Example of item in the training data\n",
    "print(\"Training sentence example :\", \" \".join([word_dict.id_to_word(id) for id in lstm_train_dataset[123][0].tolist()]), \" ; Target sentence example :\", \" \".join([word_dict.id_to_word(id) for id in lstm_train_dataset[123][1].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Class for the LSTM module, based on the Pytorch based class.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embed_size:int, hidden_size:int, dropout_prob:float=0.3) -> None:\n",
    "        \"\"\"Initialize the LSTM\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of words in the dictionnary\n",
    "            embed_size (int): Embedding size\n",
    "            hidden_size (int): Size of the hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def variational_dropout(self, input:Tensor) -> Tensor:\n",
    "        if self.training: # Apply variational dropout only during training\n",
    "            mask = (torch.rand_like(input) > self.dropout_prob).float().to(input.device)\n",
    "            mask = mask.div_(1.0 - self.dropout_prob)\n",
    "            return input * mask\n",
    "        return input # If not in training, return input\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input of the model (batched), size (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: output of the model, in the form of a (vocab_size, x.shape(1)) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        embeds = self.embeddings(x) # (batch_size, seq_len, embed_size)\n",
    "        embeds = self.variational_dropout(embeds) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        output, _ = self.lstm(embeds) # (batch_size, seq_len, hidden_size)\n",
    "        output = self.variational_dropout(output) # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        output = self.fc(output) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 7.29, Dev Loss: 6.45\n",
      "Epoch [2/20], Train Loss: 6.33, Dev Loss: 6.39\n",
      "Epoch [3/20], Train Loss: 6.21, Dev Loss: 6.33\n",
      "Epoch [4/20], Train Loss: 6.10, Dev Loss: 6.27\n",
      "Epoch [5/20], Train Loss: 6.00, Dev Loss: 6.23\n",
      "Epoch [6/20], Train Loss: 5.91, Dev Loss: 6.19\n",
      "Epoch [7/20], Train Loss: 5.84, Dev Loss: 6.15\n",
      "Epoch [8/20], Train Loss: 5.77, Dev Loss: 6.14\n",
      "Epoch [9/20], Train Loss: 5.71, Dev Loss: 6.11\n",
      "Epoch [10/20], Train Loss: 5.65, Dev Loss: 6.10\n",
      "Epoch [11/20], Train Loss: 5.59, Dev Loss: 6.10\n",
      "Epoch [12/20], Train Loss: 5.54, Dev Loss: 6.09\n",
      "Epoch [13/20], Train Loss: 5.50, Dev Loss: 6.09\n",
      "Epoch [14/20], Train Loss: 5.45, Dev Loss: 6.09\n",
      "Epoch [15/20], Train Loss: 5.41, Dev Loss: 6.08\n",
      "Epoch [16/20], Train Loss: 5.37, Dev Loss: 6.08\n",
      "Epoch [17/20], Train Loss: 5.34, Dev Loss: 6.08\n",
      "Epoch [18/20], Train Loss: 5.30, Dev Loss: 6.09\n",
      "Epoch [19/20], Train Loss: 5.28, Dev Loss: 6.09\n",
      "Epoch [20/20], Train Loss: 5.23, Dev Loss: 6.10\n"
     ]
    }
   ],
   "source": [
    "# Model and training parameters\n",
    "LSTM_EMBED_SIZE = 128\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "LSTM_BATCH_SIZE = 32\n",
    "LSTM_EPOCHS = 20\n",
    "\n",
    "# DataLoader\n",
    "lstm_train_loader = DataLoader(lstm_train_dataset, batch_size=LSTM_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "lstm_dev_loader = DataLoader(lstm_dev_dataset, batch_size=LSTM_BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "vocab_size = len(word_dict)\n",
    "lstm_model = LSTMModel(vocab_size, LSTM_EMBED_SIZE, LSTM_HIDDEN_SIZE)\n",
    "lstm_criterion = nn.CrossEntropyLoss(ignore_index=word_dict.word_to_id('<pad>')) # We can see the problem as a multi-label classification problem, classyfying among all possibles words in the vocab\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(LSTM_EPOCHS):\n",
    "    lstm_model.train()\n",
    "    lstm_train_loss = 0\n",
    "\n",
    "    # Train loop\n",
    "    for context, target in lstm_train_loader:\n",
    "\n",
    "        # Forward pass\n",
    "        output = lstm_model(context)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output = output.view(-1, vocab_size) # (batch_size *  seq_len, vocab_size)\n",
    "        target = target.view(-1) # (batch_size * seq_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = lstm_criterion(output, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        lstm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "        lstm_train_loss += loss.item()\n",
    "    \n",
    "    # Dev loop\n",
    "    lstm_model.eval()\n",
    "    lstm_dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in lstm_dev_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            output = lstm_model(context)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, vocab_size) # (batch_size *  seq_len, vocab_size)\n",
    "            target = target.view(-1) # (batch_size * seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = lstm_criterion(output, target)\n",
    "\n",
    "            lstm_dev_loss += loss.item()\n",
    "\n",
    "    lstm_train_loss /= len(lstm_train_loader)\n",
    "    lstm_dev_loss /= len(lstm_dev_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{LSTM_EPOCHS}], \"\n",
    "            f\"Train Loss: {lstm_train_loss:.2f}, \"\n",
    "            f\"Dev Loss: {lstm_dev_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic generated sentence : i m not a lot of the same\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "lstm_model.eval()\n",
    "\n",
    "context = [\"<bos>\"]\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "MAX_LEN = 25\n",
    "\n",
    "generated_sequence = context[:]\n",
    "while generated_sequence[-1] != \"<eos>\" and len(generated_sequence) <= MAX_LEN:\n",
    "    context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence], dtype = torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = lstm_model(context_indices)[:, -1]\n",
    "        probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "    predicted_index = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_word = word_dict.id_to_word(predicted_index)\n",
    "    generated_sequence.append(predicted_word)\n",
    "\n",
    "print(\"Deterministic generated sentence :\", \" \".join(generated_sequence[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random generated sentence : name hopefully bud , it 's awful , it 's allowed but small cute to 2018 the people fell\n",
      "Random generated sentence : it was n't did no a means on getting enough knowledge participants this was saying though\n",
      "Random generated sentence : wow , i saw the world ca\n",
      "Random generated sentence : i think it was kind of luck in the hires when you 're gainfully only that is a like an island women ghosting would\n",
      "Random generated sentence : i'm the authors of 35 sorry you re wo n't see so enough haha better in it\n",
      "Random generated sentence : drugs are have awful in a for name name\n",
      "Random generated sentence : we are interest\n",
      "Random generated sentence : the fuck sunny\n",
      "Random generated sentence : hello mate and spending very conservative ! !\n",
      "Random generated sentence : false is okay i hope it makes me feel less\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "\n",
    "lstm_model.eval()\n",
    "\n",
    "context = [\"<bos>\"]\n",
    "context_indices = torch.tensor([word_dict.word_to_id(word) for word in context], dtype = torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    generated_sequence = context[:]\n",
    "    while generated_sequence[-1] != \"<eos>\" and len(generated_sequence) <= MAX_LEN:\n",
    "        context_indices = torch.tensor([word_dict.word_to_id(word) for word in generated_sequence], dtype = torch.long).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = lstm_model(context_indices)[:, -1]\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "\n",
    "        predicted_index = torch.multinomial(probabilities.squeeze(), num_samples=1).item()\n",
    "        predicted_word = word_dict.id_to_word(predicted_index)\n",
    "        generated_sequence.append(predicted_word)\n",
    "\n",
    "    print(\"Random generated sentence :\", \" \".join(generated_sequence[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "    \"\"\"Perplexity computation\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Init method.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Rset method.\n",
    "        \"\"\"\n",
    "        self.log_sum = 0\n",
    "        self.total_words = 0\n",
    "        self.log_sum_list = []\n",
    "\n",
    "    def add_sentence(self, log_probs) -> None:\n",
    "        \"\"\"Compute values for one sentence and store it in the class.\n",
    "        \"\"\"\n",
    "        self.log_sum += log_probs\n",
    "        self.total_words += 1\n",
    "        self.log_sum_list += [self.log_sum]\n",
    "\n",
    "    def compute_perplexity(self) -> float:\n",
    "        \"\"\"Compute full Perplexity\n",
    "\n",
    "        Returns:\n",
    "            float: Final perplexity\n",
    "        \"\"\"\n",
    "        return math.exp(-self.log_sum / self.total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Model Perplexity : 96.34\n"
     ]
    }
   ],
   "source": [
    "ngram_model.eval()\n",
    "perplexity_object = Perplexity()\n",
    "\n",
    "# Dataset & DataLoader\n",
    "ngram_test_dataset = NGramDataset(train_data, word_dict, CONTEXT_SIZE)\n",
    "ngram_test_loader = DataLoader(ngram_test_dataset, batch_size=NGRAM_BATCH_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context, target in ngram_test_loader:\n",
    "        output = ngram_model(context)\n",
    "        probs = torch.log_softmax(output, dim=1)\n",
    "        for i in range(len(target)):\n",
    "            perplexity_object.add_sentence(probs[i, target[i]].item())\n",
    "\n",
    "perplexity = perplexity_object.compute_perplexity()\n",
    "\n",
    "print(f\"N-Gram Model Perplexity : {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown words in test set: 0\n"
     ]
    }
   ],
   "source": [
    "unk_count = sum(1 for _, next_word in ngram_test_dataset if next_word.item() == word_dict.word_to_id(\"<unk>\"))\n",
    "print(f\"Number of unknown words in test set: {unk_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: , but other than that ; True next word: <eos> ; Probability: 0.05028929561376572 ; Maximum Probability: 0.05028929561376572 ; Predicted word: <eos>\n",
      "Context: little improvements , something slightly ; True next word: different ; Probability: 0.0010384854394942522 ; Maximum Probability: 0.050557348877191544 ; Predicted word: <eos>\n",
      "Context: i called them turd gen ; True next word: , ; Probability: 0.1950800120830536 ; Maximum Probability: 0.23016342520713806 ; Predicted word: <eos>\n",
      "Context: oh , totally mistaken thanks ; True next word: for ; Probability: 0.256400465965271 ; Maximum Probability: 0.256400465965271 ; Predicted word: for\n",
      "Context: in a lot further than ; True next word: i ; Probability: 0.01578563265502453 ; Maximum Probability: 0.03158121928572655 ; Predicted word: <eos>\n",
      "Context: do n't understand humor imagine ; True next word: living ; Probability: 0.0006788246682845056 ; Maximum Probability: 0.05318064242601395 ; Predicted word: the\n",
      "Context: <bos> <bos> <bos> <bos> 2 ; True next word: is ; Probability: 0.011139093898236752 ; Maximum Probability: 0.02515491098165512 ; Predicted word: ,\n",
      "Context: <bos> <bos> <bos> <bos> <bos> ; True next word: idk ; Probability: 0.0018253957387059927 ; Maximum Probability: 0.11421635001897812 ; Predicted word: i\n",
      "Context: to the character and moves ; True next word: that ; Probability: 0.012140059843659401 ; Maximum Probability: 0.03939882665872574 ; Predicted word: the\n",
      "Context: on the ideal line and ; True next word: he ; Probability: 0.022312620654702187 ; Maximum Probability: 0.17370574176311493 ; Predicted word: i\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.random.choice(len(ngram_train_dataset), 10):\n",
    "    context, next_word = ngram_train_dataset[i]  \n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(ngram_model(context.unsqueeze(0)), dim=1)\n",
    "        prob = probs[0, next_word].item()\n",
    "        max_prob = torch.max(probs[0])\n",
    "        max_word = torch.argmax(probs[0])\n",
    "        context_sentence = \" \".join([word_dict.id_to_word(id) for id in context.tolist()])\n",
    "        print(f\"Context: {context_sentence} ; True next word: {word_dict.id_to_word(next_word.item())} ; Probability: {prob} ; Maximum Probability: {max_prob.item()} ; Predicted word: {word_dict.id_to_word(max_word.item())}\")\n",
    "        if prob == 0:\n",
    "            print(f\"Zero probability for context: {context} and next word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
